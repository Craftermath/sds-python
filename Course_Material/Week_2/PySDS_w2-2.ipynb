{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2. Text processing and file input output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1. Introduction to File types \n",
    "\n",
    "For each of the following file types, we will have a bit of a summary of the file type, it's relevance and it's structure. Then we will discuss about how to get the file type into and out of a DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## JSON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XML\n",
    "\n",
    "XML stands for 'extensible mark-up language'. XML files can be generic or have a document type. For exmaple, GraphML is really just XML with a specific schema that is used for social network graph types. \n",
    "\n",
    "Like HTML, XML is a markup language with less than and greater than to encase the element tags. The text inside these tags must have some special characters escaped. \n",
    "\n",
    "~~~ xml \n",
    "<start> \n",
    "    <middle>\n",
    "        <end1>   Here is an element! </end1>\n",
    "        <end2>   Here is an element! </end2>\n",
    "    </middle>\n",
    "</start>\n",
    "~~~\n",
    "\n",
    "Elements have an \"element tree\". Above, ```start``` is the root node, ```middle``` is a child and ```end1``` is a child of middle. ```end1``` and ```end2``` are siblings. \n",
    "\n",
    "XML is a self-documenting style, which means that you can insert details about the elements into the document itself. This can be accomplished with keys that are often prepended to the top of the document just below any details about the formatting. \n",
    "\n",
    "Most of the time, we will not be so concerned with the top of an XML document. Rather, we will simply want to navigate the element tree to get to the element(s) that are of concern to us. \n",
    "\n",
    "In the script below, we will use urllib to request an XML document from Wikipedia. Then we will use a module called 'beautiful soup' to navigate the document and return aspects of the XML.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bytestreams \n",
    "\n",
    "Sometimes we will want to use bytestreams in order to read in data. It is not very common, but for example when reading in zipfiles by code, it is important. Below is an example of reading in a file and then as a bytestream to see the difference. \n",
    "\n",
    "Bytestreams are **encoded** character sets. (i.e. they are written in the code computers understand). Strings have been **decoded** so that they can be printed for people to read. Depending on your operating system, you might not be able to write a file. We typically want to decode to UTF-8 which will write the file with the code points that a computer can use to decode the file when it needs to represent the character to a user. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This program may have difficulty encoding the emoji\n",
      "Below we are reading as a string a file that has been encoded\n",
      "hello ðŸ‘‹\n",
      "Below we are reading as a byte and then decoding it\n",
      "hello 👋\n"
     ]
    }
   ],
   "source": [
    "x = \"hello 👋\"\n",
    "\n",
    "try: \n",
    "    fileout = open(\"temp.txt\",'w')\n",
    "    fileout.write(x)\n",
    "    fileout.close()\n",
    "except UnicodeEncodeError: \n",
    "    print(\"This program may have difficulty encoding the emoji\")\n",
    "\n",
    "fileout = open(\"temp.txt\",'w',  encoding='UTF-8')\n",
    "fileout.write(x)\n",
    "fileout.close()\n",
    "\n",
    "print(\"Below we are reading as a string a file that has been encoded\")\n",
    "filein = open(\"temp.txt\",'r')\n",
    "print(filein.read())\n",
    "filein.close()\n",
    "\n",
    "print(\"Below we are reading as a byte and then decoding it\")\n",
    "filein = open(\"temp.txt\",'rb')\n",
    "print(filein.read().decode('UTF-8'))\n",
    "filein.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialization\n",
    "\n",
    "Sometimes, you want to close a program and pick up right where you left off. This might mean ensuring that all the objects are in the state that you want them to be with no further processing. This process of creating a file that will represent the state of some values is called serialization. We 'serialize' variables or data structures. \n",
    "\n",
    "Now, python being python, they had to give it a more friendly name. So in python if you want to save the state of a variable or set of variables as is, you can 'pickle' them. You can then 'unpickle' them later on. \n",
    "\n",
    "One useful approach with pickling is when you are processing text on a server and you are doing something complicated, you can pickle all your current state of things if the program goes sour, then pick up where you left off.\n",
    "\n",
    "You can only serialize one object at a time, but of course that object can be a collection of numerable other objects. Since these files are meant for  This is done with the following syntax: \n",
    "\n",
    "~~~ py \n",
    "import pickle \n",
    "x = <object> \n",
    "pickle.dump(x,open(<file>,'wb')) \n",
    "~~~\n",
    "\n",
    "And to load the object again (with any name):\n",
    "~~~\n",
    "y = pickle.load(open(<file>), 'rb')\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "x = ['1','2']\n",
    "pickle.dump(x,open(\"temp.txt\",'wb'))\n",
    "x = pickle.load(open(\"temp.txt\",'rb'))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickles can expire: Check the version number\n",
    "\n",
    "Notice that we are using 'rb' and 'wb' with the pickles. This is because Python 3's default pickling version writes the pickled object as a bytestream. Also note that this pickled object will probably not be readable by python 2.  You can set the pickle protocol so that it is readable by python 2 as an option. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV \n",
    "\n",
    "Comma-separated values is a common data storage format. Yet, despite it's prevalence, there are a few variations to consider:\n",
    "- How are strings represented? Do they use \"<string>\" for every string, no string or only strings with commas in them? \n",
    "- How are new lines represented? \n",
    "- Is there a header? \n",
    "- Is there a trailing \\n?\n",
    "\n",
    "It is simple to roll your own csv reader. So much so that you did a version of this on the first day. Yet, there are often enough details to consider that you might want to rely on an external program. Python will offer two. First is the ```csv``` module. This is a standalone package that can be imported. It has many options for separators and whether there's a header. It also has some nice ways to index the data. For example, if you want to store your data as a dictionary with the header as the key and the column as the values, this does the trick. \n",
    "\n",
    "The basic usage, however, is to iterate through a file line by line. Instead of iterating through with 'readline' and splitting the text that comes back, you create a \"csv reader\", and this iterates line by line returning not a string of text, but a list split at every comma (or user-defined separator). \n",
    "\n",
    "~~~ python \n",
    "import csv \n",
    "\n",
    "with open('data.csv', newline='') as file_to_read:\n",
    "    filereader = csv.reader(file_to_read, delimiter=' ', quotechar='|')\n",
    "    for row in filereader:\n",
    "        print('<>'.join(row))\n",
    "~~~\n",
    "\n",
    "The nice thing about ```csv```, particularly when not using pandas, is the use of the DictReader. This returns a dictionary with the header as the key and the values in that row as the value. If there's no header line, you can specify a list as ```fieldnames```. \n",
    "\n",
    "The second is simply reading in using pandas default importer. \n",
    "\n",
    "~~~\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(<path_to_file>) \n",
    "\n",
    "or xx\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excel \n",
    "\n",
    "Excel is the popular spreadsheet program from Microsoft. Files can be stored as either .xls or .xlsx. The first one is a bytestream proprietary file format, but the details are handled by PANDAS. The second one was published as an open standard and is in fact a wrapper over a specific format of xml.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for our DataFrames\n",
    "import pandas as pd\n",
    "\n",
    "# This package allows us to parse json\n",
    "import json\n",
    "\n",
    "# This package allows us to take a list in json and turn it into a DataFrame\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "# This package allows us to download data from the web\n",
    "import urllib\n",
    "\n",
    "# This is a package called \"Beautiful Soup\" that parses webpages/XML and it available as an object\n",
    "import bs4\n",
    "\n",
    "# This package stops time so you can work on your assignment\n",
    "# Just kidding, it allows you to pause a program\n",
    "import time\n",
    "\n",
    "# This package allows you to use plt to plot data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This package allows you to use regular expressions\n",
    "import re\n",
    "\n",
    "# This magic command places image output in the workbook\n",
    "# If you want to save a figure to your computer use plt.savefig(PATH)\n",
    "%pylab inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review from last week\n",
    "\n",
    "In python we can create data structures that we can use for managing data, filtering it and combining it in new ways. We introduced the **Series** data structure that has properties of both lists (ordered, indexable) and dictionaries (queryable by key). We also introduced the **DataFrame** which is like a collection of Series as columns, with rows as cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic file manipulation\n",
    "-----------------------\n",
    "\n",
    "File manipulation consists of creating file openers and then working with those openers. The openers take two arguments. The first is the path to the file. If there is no path given beyond the file name, it will assume that the file is in the same directory as the code. \n",
    "\n",
    "    fileopener = open(\"path\",\"r/w/a\")\n",
    "\n",
    "The second argument is either 'r', 'w' or 'a'. These respectively refer to read, write and append. Once you have a file opener you can either read the file or write the file. \n",
    "\n",
    "**Reading from a file**\n",
    "\n",
    "When reading a file, you can either read the entire contents with the read() command:\n",
    "\n",
    "    entire_file = fileopener.read()\n",
    "    \n",
    "You can also read line by line using an iterator. \n",
    "\n",
    "    for line in fileopener:\n",
    "        print(line)\n",
    "\n",
    "Remember! When you do this, each line will be returned. These lines typically have a new line character at the end of the line. So when you print it, you will print a space in between these lines. The way to get rid of that character is to \"strip\" it from the text:\n",
    "\n",
    "    for line in fileopener:\n",
    "        print(line.strip())\n",
    "\n",
    "This will always start from the cursor. So if you have already read the file, you will either have to re-open it or set the cursor back to the original position. \n",
    "\n",
    "    fileopener.read()\n",
    "    fileopener.seek(0)\n",
    "    for line in fileopener:\n",
    "         print(line)\n",
    "\n",
    "When you are done you should close the file:\n",
    "\n",
    "    fileopener.close()\n",
    "    \n",
    "** Writing to a File **\n",
    "\n",
    "Writing to a file involves again creating an opener. The command to write to a file is:\n",
    "\n",
    "\n",
    "    fileopener.write(STRING)\n",
    "     \n",
    "When you are done writing to the file it is also a good idea to close the file. Again, it is:\n",
    "\n",
    "    file.close()\n",
    "\n",
    "This closing is less important for reading than it is for writing. Why? Because writing to the file doesn't always ensure that the physical harddrive is written. By closing the file, it will 'flush' the contents of the file to the hard disk. If in the case you think it's important to periodically flush the data to the hard disk this is possible, too:\n",
    "\n",
    "    fileopener.flush()\n",
    "\n",
    "WARNING! Creating a writable file open actually creates a pointer to a file on the computer using the name of the file. This is especially important to understand because you can run the risk of destroying another file with the same name. This is what is called \"clobbering\". When you work in a word processor and try to save a file with the same name the computer will ask you 'file name already exists. are you sure?' or something to that effect. Python will not give such a warning. So please be careful with the names of your file openers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"hello world\"\n",
    "FILENAME = \"example_clobber.txt\"\n",
    "fileout =  open(FILENAME, \"w\")\n",
    "fileout.write(txt)\n",
    "fileout.close()\n",
    "\n",
    "filein = open(FILENAME)\n",
    "print(filein.read())\n",
    "\n",
    "filein.close()\n",
    "\n",
    "fileout = open(FILENAME, 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#############################################\n",
    "# FILE MANIPULATION EXAMPLES \n",
    "\n",
    "NAME = \"EXAMPLE_FILE\"\n",
    "\n",
    "# Write a file \n",
    "fileout = open(\"%s_testOut.txt\" % (NAME),\"w\")\n",
    "fileout.write(\"yes!\\n\"*10)\n",
    "fileout.close()\n",
    "\n",
    "# Read a file \n",
    "filein = open(\"%s_testOut.txt\" % (NAME), 'r')\n",
    "print(\"Begin printing in full\")\n",
    "\n",
    "print(filein.read())\n",
    "\n",
    "print(\"Begin printing line by line\")\n",
    "\n",
    "filein.seek(0) # this resets the cursor to the beginning of the file. \n",
    "\n",
    "for count,line in enumerate(filein):\n",
    "    print(count,\":\",line.strip())\n",
    "    \n",
    "# for line in filein: \n",
    "#     print(line)\n",
    "    \n",
    "filein.close()\n",
    "print(\"***File closed***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append a file \n",
    "fileout = open(\"%s_testOut.txt\" % NAME,\"a\")\n",
    "fileout.write(\"No!\")\n",
    "fileout.close()\n",
    "\n",
    "filein = open(\"%s_testOut.txt\" % NAME,\"r\")\n",
    "\n",
    "for count,line in enumerate(filein):\n",
    "    print(count,\":\",line)\n",
    "\n",
    "filein.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing bytestrings and serializing files\n",
    "-----------------------------------------\n",
    "\n",
    "Sometimes you want to store some python objects exactly as the program is using them. This is called serialization in programming parlance. In python, this is called \"pickling\", as in the way one preserves things by putting them in jars with spices. In the simples instance, you merely want to call the \"dump\" or \"load\" methods. However, with Python 3, this works slightly differently than before. Now instead of writing the text as you read it, you have to write it as a stream of bytes. Go to a text editor and open a .jpeg or a .mp3 and you will see what a bytestring looks like. They are pretty unintelligible to humans but they work just fine for the computer. It is often a good idea to pickle some work if you are on a server and concerned what would happen if the program failed. \n",
    "\n",
    "To create a file opener as a bytestring, you have to append 'b' to the read or write argument\n",
    "\n",
    "    open(PATH, 'wb') \n",
    "    \n",
    "So the resulting code to open a file for pickling is\n",
    "\n",
    "    pickle.dump(PYTHON_OBJECT, open(PATH, 'wb')\n",
    "    \n",
    "To read from the file (and typically assugn it to a variable:\n",
    "\n",
    "    new_object = pickle.load(open(PATH, 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Write a pickle \n",
    "x = [\"best\",\"script\",\"ever\"]\n",
    "y = {\"comic book guy\": \"best . script . ever.\"}\n",
    "\n",
    "z = (x,y)\n",
    "\n",
    "pickle.dump(z,open(\"testPickle.pkl\",'wb'))\n",
    "\n",
    "new_z = pickle.load(open(\"testPickle.pkl\",'rb'))\n",
    "print(new_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON\n",
    "\n",
    "JSON stands for javascript object notation. It is a common lightweight form for data collection. We can see json on the web really easily. Just navigate to reddit, as in one of the nicer reddits, such as www.reddit.com/aww then append .json to the end of the URL. See there! It's basically just lists and dictionaries. You can load json data with \n",
    "\n",
    "~~~\n",
    "import json\n",
    "\n",
    "datastructure = json.loads(THE_DATA)\n",
    "~~~\n",
    "\n",
    "Then you can just query it as a series of nested lists and dictionaries. You can also print this data in a nice readable format (called pretty printing) in the following way: \n",
    "\n",
    "~~~\n",
    "json.dumps(THE_DATA, indent=4) \n",
    "~~~\n",
    "\n",
    "There are nicer ways to pretty print. Let's look at json from Reddit pretty printed. Open a browser window and head to:\n",
    "\n",
    "https://jsonformatter.curiousconcept.com \n",
    "\n",
    "In there type: \n",
    "\n",
    "http://reddit.com/r/aww.json \n",
    "\n",
    "Notice how you can collapse and expand the json file. We will use this to navigate through the file then download it for processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here's an example snippet. \n",
    "# We are going to download from the aww subreddit, then use json_normalize to stick it right in a DataFrame\n",
    "# 1.\n",
    "SUBREDDIT = \"aww\"\n",
    "URL = \"http://www.reddit.com/r/%s.json\" % SUBREDDIT\n",
    "\n",
    "# URL queries are preceded by a header. Your browser has a header, too. \n",
    "# Check: https://www.whatismybrowser.com/detect/what-http-headers-is-my-browser-sending\n",
    "# Part of that header is the \"user agent\" string. Mine is: \n",
    "# Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.89 Safari/537.36 OPR/49.0.2725.39\n",
    "# Reddit expects a string other than the python default. \n",
    "# You should alter it to be unique to you. \n",
    "# 1/0 # DELETE ME AFTER YOU'VE CHANGED THE HEADER\n",
    "# 2. \n",
    "req = urllib.request.Request( URL, headers={'User-Agent': 'OII class 2018.1/Hogan'})\n",
    "\n",
    "# IF you make repeated queries to Reddit, you must pause between them for a minimum of three seconds. \n",
    "# This snippet (time.sleep(3)) will do that for you, you just have to \"import time\" \n",
    "# time.sleep(4)\n",
    "# 3. \n",
    "infile = urllib.request.urlopen(req)\n",
    "\n",
    "# Here THE_DATA is we read the result and decode it. \n",
    "# 4. \n",
    "redditData = json.loads(infile.read().decode('utf8'))\n",
    "\n",
    "# We we just say take the JSON and make a table. \n",
    "# 5. \n",
    "rtable = json_normalize(redditData[\"data\"][\"children\"])\n",
    "rtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we have a DataFrame with at least two numerical columns, we can \n",
    "# plot it as a scatter plot. \n",
    "plt.scatter(rtable['data.ups'],rtable['data.num_comments'])\n",
    "\n",
    "# We can add a label to the x-axis, the y-axis and the entire table. \n",
    "plt.xlabel('Upvotes')\n",
    "plt.ylabel('Comments')\n",
    "plt.title('Reddit upvotes by comments in r/%s' % subreddit)\n",
    "\n",
    "plt.savefig(\"test.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XML. Navigating nested mark-up language\n",
    "------------------------------------------\n",
    "\n",
    "Both XML and HTML are examples of mark-up **languages** with a DOM tree. That is the documents follow a hierarchical structure and use mark-up tags to indicate which part of the structure you are in. The tags that denote the sturcture are in brackets. Open tags have a word (and some options), closing tags have the same word but with a / in front of it. Here is a basic HTML document\n",
    "\n",
    "    <html>\n",
    "        <head>\n",
    "            <title> \n",
    "                This is the title! \n",
    "            </title>\n",
    "        </head>\n",
    "        <body>\n",
    "            This is a webpage!\n",
    "        </body>\n",
    "    </html>\n",
    "\n",
    "There are a number of programs that will convert a raw HTML or XML document into a python object that can be navigated. In my opinion, one of the nicest is the package \"BeautifulSoup\". It takes a bit of getting used to, but it will be of significant help. We can start by downloading an HTML or XML document and then parsing it. The Anaconda package should have beautifulsoup embedded.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can set this Wikipage to be any string that has a wikipedia page.\n",
    "WIKIPAGE = \"United Kingdom\"\n",
    "\n",
    "# Here we use urllib.parse.quote to turn spaces and special characters into\n",
    "# the characters needed for an html string. So for example spaces become %20\n",
    "URL = \"http://en.wikipedia.org/wiki/Special:Export/%s\" % urllib.parse.quote(WIKIPAGE)\n",
    "\n",
    "# View the output here to see how quote strings get formatted. \n",
    "# You don't need to remember the codes, just that you typically need to \"quote\" before\n",
    "# requesting from a browser. \n",
    "EXAMPLE = \"Here are some quoted strings:\\n!_@_#_$_%_^_&_*_(_)_-_=_+_/_?\" \n",
    "qEXAMPLE = urllib.parse.quote(EXAMPLE)\n",
    "print(EXAMPLE,\"\\n\\n\",qEXAMPLE)\n",
    "\n",
    "print(URL,\"\\n\")\n",
    "# Let's look at this page in an XML browser. Copy and paste it into your browser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, don't forget the header! \n",
    "req = urllib.request.Request( URL, headers={'User-Agent': 'OII class 2017.1/Hogan'})\n",
    "\n",
    "# This is the data we receive by opening the URL. But it's not the file, it's just a pointer. \n",
    "infile = urllib.request.urlopen(req)\n",
    "\n",
    "wikitext = infile.read()\n",
    "# print(wikitext)\n",
    "# print(\"#######\\n\\n\\n\\n\\n######\\n\")\n",
    "# print(wikitext.decode('utf8'))\n",
    "\n",
    "# This does a lot of things. \n",
    "# infile.read().decode('utf8') >> reads the page but assumes the page is unicode, not plain ASCII\n",
    "# soup = bs4.BeautifulSoup(TEXT, features=\"xml\") >> this converts the text to a \"soup\" that can be queried.\n",
    "#                                                   By saying features=\"xml\", the soup knows how to parse it. \n",
    "#\n",
    "soup = bs4.BeautifulSoup(wikitext.decode('utf8'), \"lxml\")\n",
    "\n",
    "text_to_parse = soup.mediawiki.page.text\n",
    "\n",
    "# print(text_to_parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be wondering how I know that it was:\n",
    "\n",
    "    mediawiki.page.text\n",
    "    \n",
    "As we as up above:\n",
    "\n",
    "    mediawiki.page.revision.id\n",
    "\n",
    "There are two ways to learn this, the hard way and the harder way. The hard way is to just look at the raw XML and fumble around printing through the tree until you find the node that you're looking for. The \"harder\" way is to navigate through the page using the tree structure that BeautifulSoup builds. The latter way is pretty darn hard without guidance from the document itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in soup.children: print(i.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in soup.mediawiki.children: print(i.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in soup.mediawiki.page.children: print(i.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(soup.mediawiki.page.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2. Basic Text Scraping\n",
    "---------------------------\n",
    "\n",
    "Basic text scraping is the practice of taking some data and cleaning it in such a way that it can be used for other programs. Below are a series of excercises designed to help you understand the fundamentals of text processing. In particular, we will focus on the process of handling whitespace. This will involve using several additional files that should be uploaded to your workspace. \n",
    "\n",
    "1. Cleaning up by line breaks. \n",
    "2. Splitting text by space. \n",
    "3. Finding specific words and characters. \n",
    "4. Converting from one character set to another character sets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2.1 - Stripping characters.**\n",
    "\n",
    "Below we will take a file, read it, print it and then get rid of the return characters. Please pay attention to the line-breaks when the file is being printed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"example_lines.txt\") as file:\n",
    "    for i in file:\n",
    "        print(i.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleaning up the lines**\n",
    "Did you notice that each of the lines has a space in between them? This is because we printed:\n",
    "\n",
    "    \"Testing Line 1\\n\"\n",
    "    \n",
    "And this is becuase when python reads the file it does it line by line. It splits the file at the new line character but keeps that character in the string when it returns the string. To get rid of these new line characters we would **strip()** the whitespace from the ends of the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"example_lines.txt\") as file:\n",
    "    for i in file:\n",
    "        print(i.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "with open(\"example_lines.txt\") as file:\n",
    "    for i in file:\n",
    "        words += i.split()\n",
    "        \n",
    "# print(words)\n",
    "\n",
    "wordseries = pd.Series(words)\n",
    "display(wordseries.value_counts())\n",
    "for i in list(wordseries.value_counts()[wordseries.value_counts() > 1].index):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word frequency**\n",
    "\n",
    "So, as we can see above, we have all sorts of issues with words. The word 'line' is there in upper and lower case, sometimes the text uses numbers, sometimes it has periods in there. We can do all sorts of things to these data to  clean them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "with open(\"example_lines.txt\") as file:\n",
    "    for i in file:\n",
    "        words = i.split()\n",
    "        for j in words:\n",
    "            j = j.lower() # all words are now lower case\n",
    "\n",
    "            try: \n",
    "                if not j[-1].isalpha(): j = j[:-1] # non-alpha suffix\n",
    "                if not j[0].isalpha(): j = j[1:] # non-alpha prefix\n",
    "                if len(j) <= 1: continue # empty strings\n",
    "\n",
    "            except IndexError:\n",
    "                    continue\n",
    "                \n",
    "            # Once cleaned, we can then add the words to a dictionary \n",
    "            # The word will be the 'key' and the frequency will be the 'value'\n",
    "            if j in word_dict: word_dict[j] += 1\n",
    "            else: word_dict[j] = 1\n",
    "\n",
    "print(pd.Series(word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series(word_dict,name=\"Count\")\n",
    "print(data.value_counts())\n",
    "\n",
    "data.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3. Simple regular expressions\n",
    "--------------------------------\n",
    "\n",
    "\"Regular expressions\" are pieces of text that can be expressed in a regular form even if the characters are different. For example, when you encounter a URL on a webpage and right click on it, the browser knows that this is a URL and asks \"open link in new tab\". It does not need to know every URL, just what URLs are supposed to look like (that is they start with \"HTTP://\" or \"HTTPS://\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "example_text = \"1234\\t3333\\t10000\\t1,500,442\\t3.14\"\n",
    "print(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just find the numbers\n",
    "reg01 = re.compile(\"[0-9]\")\n",
    "print(reg01.findall(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just find the numbers\n",
    "reg01 = re.compile(\"walk[\\s]*\")\n",
    "print(reg01.findall(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hmm...it seems * matches 0 or more instances \n",
    "reg01 = re.compile(\"[0-9]+\")\n",
    "print(reg01.findall(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's deal with those commas\n",
    "reg01 = re.compile(\"[\\d,.]+\")\n",
    "print(reg01.findall(example_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the above examples, working with regexs involve compiling a 'regular expression' and then applying that to text. Obviously, we could have just split on the tab character in this particular instance, but it's the logic of specifying regexs that's important, such as saying \"all digits\" or \"one or more digits plus a comma.\" In the parentheses for the regular expression we can either ask for 0,1,n or a predetermined number of characters. The characters can be in a range, such as 0-9 or a-z. But we can also use escape codes for the characters. See below for examples of regexs with text.  \n",
    "\n",
    "Also, as a hint, if you forgot about using the **help()** command "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_string = example_text.replace(\",\",\"\")\n",
    "print(\"Old string: \", example_text)\n",
    "print(\"New string: \", new_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"😱 hello fellow kids, ur 2edgy4me 😝😝😝; you're like 3edgy5me. \" + \\\n",
    "                \"Yh, I replaced the 2 with a 3.\\nI'm that edgy 😈.\"\n",
    "print(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg02 = re.compile(\"[a-z]+\")\n",
    "print(reg02.findall(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg02 = re.compile(\"\\S+\")\n",
    "print(reg02.findall(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg02 = re.compile(\"[😱😝😈🎅🏾🙈]\")\n",
    "emojilist = reg02.findall(example_text)\n",
    "print(emojilist)\n",
    "\n",
    "emojiset = set(emojilist)\n",
    "print(emojiset)\n",
    "\n",
    "print(pd.Series(emojilist).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Returning to the XML above with regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.mediawiki.page.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the text we can look through it for regularly formatted text. This is ideal for Wikipedia since it is a wiki. Wikis use regularly formatted text for all of its features, and Wikipedians are keen to make sure that the page is formatted properly. It should come as no surprise that MediaWiki itself uses a ton of regular expressions to parse the wiki text in the first place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes you have done the above code\n",
    "re_inner_links = re.compile(r'\\[\\[.*?\\]\\]')\n",
    "re_outer_links = re.compile(r'https?://[\\w\\./?&=%]*')\n",
    "inner_links = re_inner_links.findall(text_to_parse)\n",
    "# outer_links = re_outer_links.findall(text_to_parse)\n",
    "# print(inner_links)\n",
    "# print(\"The program found %s wikilinks, of which %s are unique.\" % (len(inner_links),len(set(inner_links))))\n",
    "print(pd.Series(inner_links).value_counts()[pd.Series(inner_links).value_counts() > 1])\n",
    "\n",
    "# print(inner_links)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
